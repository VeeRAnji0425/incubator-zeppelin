{
  "paragraphs": [
    {
      "text": "%md\n\nReading data from Cassandra\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1432125137469_650840895",
      "id": "20150520-143217_474596748",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch1\u003eReading data from Cassandra\u003c/h1\u003e\n"
      },
      "dateCreated": "May 20, 2015 2:32:17 PM",
      "dateStarted": "May 20, 2015 2:33:38 PM",
      "dateFinished": "May 20, 2015 2:33:39 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n**Many ways to read data from Cassandra**:\n\n1. using plain **CassandraRDD[CassandraRow]**\n2. converting **CassandraRow** columns into **tuples**\n3. using **Scala case classes** with the integrated **object mapper**",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "tableHide": false,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1432125218563_-1408664128",
      "id": "20150520-143338_1132207280",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003e\u003cstrong\u003eMany ways to read data from Cassandra\u003c/strong\u003e:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eusing plain \u003cstrong\u003eCassandraRDD[CassandraRow]\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003econverting \u003cstrong\u003eCassandraRow\u003c/strong\u003e columns into \u003cstrong\u003etuples\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eusing \u003cstrong\u003eScala case classes\u003c/strong\u003e with the integrated \u003cstrong\u003eobject mapper\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n"
      },
      "dateCreated": "May 20, 2015 2:33:38 PM",
      "dateStarted": "May 20, 2015 2:35:15 PM",
      "dateFinished": "May 20, 2015 2:35:15 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Reset table display",
      "text": "import org.apache.spark.rdd.RDD\n\nvar data:RDD[(Int,Double)] \u003d sc.parallelize(Nil)",
      "dateUpdated": "Sep 22, 2015 3:05:25 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "title": true,
        "tableHide": true,
        "editorHide": false,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1432125938553_1725190640",
      "id": "20150520-144538_434871993",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.rdd.RDD\ndata: org.apache.spark.rdd.RDD[(Int, Double)] \u003d ParallelCollectionRDD[45] at parallelize at \u003cconsole\u003e:44\n"
      },
      "dateCreated": "May 20, 2015 2:45:38 PM",
      "dateStarted": "Sep 22, 2015 3:05:25 PM",
      "dateFinished": "Sep 22, 2015 3:05:26 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Reading data using plain CassandraRow",
      "text": "import org.apache.spark.SparkContext._\nimport com.datastax.spark.connector._\nimport com.datastax.spark.connector.rdd.CassandraRDD\nimport org.apache.spark.{SparkConf, SparkContext}\n\n\ndata \u003d sc.cassandraTable(\"spark_demo\", \"us_unemployment_stats\").\n    map( row \u003d\u003e (row.getInt(\"year\"),row.getDouble(\"unemployed_percentage_to_labor\")))",
      "dateUpdated": "Sep 22, 2015 3:06:00 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "tableHide": true,
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1432125315541_2118499250",
      "id": "20150520-143515_1959047089",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.SparkContext._\nimport com.datastax.spark.connector._\nimport com.datastax.spark.connector.rdd.CassandraRDD\nimport org.apache.spark.{SparkConf, SparkContext}\ndata: org.apache.spark.rdd.RDD[(Int, Double)] \u003d MapPartitionsRDD[47] at map at \u003cconsole\u003e:56\n"
      },
      "dateCreated": "May 20, 2015 2:35:15 PM",
      "dateStarted": "Sep 22, 2015 3:06:00 PM",
      "dateFinished": "Sep 22, 2015 3:06:02 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "read data using scala tuples",
      "text": "data \u003d sc.cassandraTable(\"spark_demo\", \"us_unemployment_stats\").\n    select(\"year\", \"unemployed_percentage_to_labor\").\n    as((_: Int, _: Double)).\n    sortByKey()",
      "dateUpdated": "Sep 22, 2015 3:06:10 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "title": true,
        "editorHide": false,
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1432126014638_1008677629",
      "id": "20150520-144654_1348217378",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "data: org.apache.spark.rdd.RDD[(Int, Double)] \u003d ShuffledRDD[51] at sortByKey at \u003cconsole\u003e:56\n"
      },
      "dateCreated": "May 20, 2015 2:46:54 PM",
      "dateStarted": "Sep 22, 2015 3:06:10 PM",
      "dateFinished": "Sep 22, 2015 3:06:11 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Read data using scala case class",
      "text": "case class UsUnemploymentRate(year: Int, unemployedPercentageToLabor: Double)\n\ndata \u003d sc.cassandraTable[UsUnemploymentRate](\"spark_demo\", \"us_unemployment_stats\").\n    sortBy(bean \u003d\u003e bean.year,true,1).\n    map(bean \u003d\u003e (bean.year,bean.unemployedPercentageToLabor))",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "title": true,
        "tableHide": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1433842506554_-1567680288",
      "id": "20150609-113506_374130940",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "defined class UsUnemploymentRate\ndata: org.apache.spark.rdd.RDD[(Int, Double)] \u003d MappedRDD[23] at map at \u003cconsole\u003e:40\n"
      },
      "dateCreated": "Jun 9, 2015 11:35:06 AM",
      "dateStarted": "Jun 17, 2015 10:31:25 AM",
      "dateFinished": "Jun 17, 2015 10:31:27 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Read data using sparkSQL",
      "text": "import org.apache.spark.sql.cassandra.CassandraSQLContext\n\nval cc \u003d new CassandraSQLContext(sc)\n\nval row \u003d cc.cassandraSql(s\"SELECT year,unemployed_percentage_to_labor \" +\n                                     s\"FROM spark_demo.us_unemployment_stats WHERE unemployed_percentage_to_labor \u003e 8 \" +\n                                     s\"ORDER BY year DESC\")\ndata \u003d row.\n    map(row \u003d\u003e (row.getInt(0),row.getDouble(1))).\n    sortBy{case(year,_) \u003d\u003e year}\n",
      "dateUpdated": "Sep 22, 2015 3:06:31 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "title": true,
        "tableHide": false,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1432126280851_1108867753",
      "id": "20150520-145120_1566314701",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.sql.cassandra.CassandraSQLContext\ncc: org.apache.spark.sql.cassandra.CassandraSQLContext \u003d org.apache.spark.sql.cassandra.CassandraSQLContext@69a2f048\nrow: org.apache.spark.sql.DataFrame \u003d [year: int, unemployed_percentage_to_labor: double]\ndata: org.apache.spark.rdd.RDD[(Int, Double)] \u003d MapPartitionsRDD[69] at sortBy at \u003cconsole\u003e:60\n"
      },
      "dateCreated": "May 20, 2015 2:51:20 PM",
      "dateStarted": "Sep 22, 2015 3:06:31 PM",
      "dateFinished": "Sep 22, 2015 3:06:35 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "data.display(\"Year\",\"Unemployment Percentage\")",
      "dateUpdated": "Sep 22, 2015 3:06:39 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "Year",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "Year",
              "index": 0.0,
              "aggr": "sum"
            }
          }
        },
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1432125489739_1001031913",
      "id": "20150520-143809_1476305925",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "Year\tUnemployment Percentage\n1941\t9.9\n1975\t8.5\n1982\t9.7\n1983\t9.6\n2009\t9.3\n2010\t9.6\n"
      },
      "dateCreated": "May 20, 2015 2:38:09 PM",
      "dateStarted": "Sep 22, 2015 3:06:39 PM",
      "dateFinished": "Sep 22, 2015 3:06:39 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Read data using dataFrames",
      "text": "sqlContext.sql(\n   \"\"\"CREATE TEMPORARY TABLE us_unemployment_stats\n     |USING org.apache.spark.sql.cassandra\n     |OPTIONS (\n     |  table \"us_unemployment_stats\",\n     |  keyspace \"spark_demo\",\n     |  cluster \"Test Cluster\",\n     |  pushdown \"true\"\n     |)\"\"\".stripMargin)\n     \nval worstYears \u003d sqlContext.sql(\"SELECT year,unemployed_percentage_to_labor \" + \n                \"FROM us_unemployment_stats \" + \n                \"WHERE unemployed_percentage_to_labor \u003e 8 \" +\n                \"ORDER BY year ASC\")          \n\nz.show(worstYears)",
      "dateUpdated": "Sep 22, 2015 3:24:32 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "year",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "unemployed_percentage_to_labor",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "year",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "unemployed_percentage_to_labor",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1442927256341_-699998091",
      "id": "20150922-150736_1092345738",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "year\tunemployed_percentage_to_labor\n1941\t9.9\n1975\t8.5\n1982\t9.7\n1983\t9.6\n2009\t9.3\n2010\t9.6\n"
      },
      "dateCreated": "Sep 22, 2015 3:07:36 PM",
      "dateStarted": "Sep 22, 2015 3:24:32 PM",
      "dateFinished": "Sep 22, 2015 3:24:33 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1437400112265_-103467212",
      "id": "20150720-154832_1356318778",
      "dateCreated": "Jul 20, 2015 3:48:32 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "02-Reading Data from Cassandra",
  "id": "2ART2CK3F",
  "angularObjects": {
    "2ARKVF46K": [],
    "2ANYRH787": [],
    "2AQBHNCAB": [],
    "2ANRWDJG1": [],
    "2ANNAMCUB": [],
    "2ARR8D6R9": [],
    "2AQAS485Z": []
  },
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}
{
  "paragraphs": [
    {
      "text": "%md\n\nHow to use Spark to clean up your data\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1432129998000_-2100208451",
      "id": "20150520-155318_1737713408",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch1\u003eHow to use Spark to clean up your data\u003c/h1\u003e\n"
      },
      "dateCreated": "May 20, 2015 3:53:18 PM",
      "dateStarted": "May 20, 2015 3:54:07 PM",
      "dateFinished": "May 20, 2015 3:54:08 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n#### **Perfect data locality** scenario: reading data from a table and writing back to the same table",
      "dateUpdated": "Sep 23, 2015 10:49:50 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorHide": true,
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1432130047273_-606720490",
      "id": "20150520-155407_1126725850",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch4\u003e\u003cstrong\u003ePerfect data locality\u003c/strong\u003e scenario: reading data from a table and writing back to the same table\u003c/h4\u003e\n"
      },
      "dateCreated": "May 20, 2015 3:54:07 PM",
      "dateStarted": "Sep 23, 2015 10:49:47 AM",
      "dateFinished": "Sep 23, 2015 10:49:48 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Reload Performers Original Data",
      "text": "%sh\n\n`which cqlsh` -e \"TRUNCATE spark_demo.performers\"\n`which cqlsh` -e \"COPY spark_demo.performers(name, country, gender, type, born, died,styles) FROM \u0027/tmp/spark_cassandra_demo/performers.csv\u0027\"",
      "dateUpdated": "Sep 23, 2015 10:52:20 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "tableHide": false,
        "title": true,
        "editorHide": false,
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1432469334336_-1771309010",
      "id": "20150524-140854_111739812",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "Processed 10000 rows; Write: 6737.96 rows/s\r\n11100 rows imported in 1.808 seconds.\n"
      },
      "dateCreated": "May 24, 2015 2:08:54 PM",
      "dateStarted": "Sep 23, 2015 10:52:20 AM",
      "dateFinished": "Sep 23, 2015 10:52:23 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Display performers born date",
      "text": "%cassandra\nSELECT born FROM spark_demo.performers LIMIT 100;\n",
      "dateUpdated": "Sep 23, 2015 10:52:26 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "born",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "born",
              "index": 0.0,
              "aggr": "sum"
            }
          }
        },
        "title": true,
        "tableHide": false,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1432130086265_7400051",
      "id": "20150520-155446_257963839",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "born\n1977\n1947-05-31\n1962-02-11\n1968-08-31\n1986\n2006\n????-01-25\n1944-06-24\nnull\n1994\n1997\n1984\n1967-12-12\n1979\nnull\n1976\n1999\n1988\n1994\n1986-04-14\n1998\nnull\n1988\n1970-06-04\nnull\n1977-06-12\n1974-02-17\n1987\n1971\nnull\nnull\n2002\n1964-09-08\n1996\n1993\n1979-02-26\n1992\n2003\nnull\n1974-07-16\n1997\n2005\nnull\nnull\nnull\n1995\nnull\n1987\n1943-11-07\n1890-12-11\n2002\n1963\n1999\nnull\n1966\n1998\n1989\n1998\n1981\n1965\n1997\n2003\n1994-10-12\nnull\nnull\n1999\n2004\nnull\n2004\n1979-09-17\n1996\nnull\nnull\n1946-12-30\nnull\nnull\n1982\n2004-04-01\n1983-08-01\nnull\nnull\n1971-12-18\nnull\n1990\n2003\n2000\nnull\n1998\nnull\n1974\n1963-11-13\n1994\n1991\n1985\nnull\n1995\n1963-04-21\n1991\n1990\n1969\n"
      },
      "dateCreated": "May 20, 2015 3:54:46 PM",
      "dateStarted": "Sep 23, 2015 10:52:26 AM",
      "dateFinished": "Sep 23, 2015 10:52:26 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "normalize performers born date",
      "text": "import com.datastax.spark.connector.rdd.CassandraTableScanRDD\nimport scala.util.matching.Regex\nimport com.datastax.spark.connector._\nimport org.apache.commons.lang3.StringUtils\nimport org.apache.spark.{SparkConf, SparkContext}\n\nval YearPattern \u003d \"^[0-9]{4}$\".r\nval YearMonthPattern \u003d \"^[0-9]{4}-[0-9]{2}$\".r    \ndef matchPattern(input:Option[String], pattern: Regex):Boolean \u003d {\n    input match {\n        case Some(value) \u003d\u003e {\n            if(StringUtils.isBlank(value)) return false\n            pattern findFirstIn value match {\n                case Some(c) \u003d\u003e true\n                case None \u003d\u003e false\n            }            \n        }\n        case None \u003d\u003e false\n    }\n}\ncase class Performer(name: String, country: String, gender:Option[String], `type`:Option[String],\n    born:Option[String],died:Option[String], styles:Set[String]) extends Serializable\n\nobject Func extends Serializable {\n\n}\n\nimport Func._\nval cache: CassandraTableScanRDD[Performer] \u003d sc.cassandraTable[Performer](\"spark_demo\", \"performers\").cache()\n\ncache.\n    filter(bean \u003d\u003e matchPattern(bean.born,YearPattern)).\n    map(bean \u003d\u003e (bean.name,bean.born.get+\"-01-01\")).\n    saveToCassandra(\"spark_demo\", \"performers\",SomeColumns(\"name\",\"born\"))\n\ncache.\n    filter(bean \u003d\u003e matchPattern(bean.born,YearMonthPattern)).\n    map(bean \u003d\u003e (bean.name,bean.born.get+\"-01\")).\n    saveToCassandra(\"spark_demo\", \"performers\",SomeColumns(\"name\",\"born\"))\n\n",
      "dateUpdated": "Sep 23, 2015 10:51:52 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "tableHide": true,
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1432130256969_-345770436",
      "id": "20150520-155736_260989089",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import com.datastax.spark.connector.rdd.CassandraTableScanRDD\nimport scala.util.matching.Regex\nimport com.datastax.spark.connector._\nimport org.apache.commons.lang3.StringUtils\nimport org.apache.spark.{SparkConf, SparkContext}\nYearPattern: scala.util.matching.Regex \u003d ^[0-9]{4}$\nYearMonthPattern: scala.util.matching.Regex \u003d ^[0-9]{4}-[0-9]{2}$\nmatchPattern: (input: Option[String], pattern: scala.util.matching.Regex)Boolean\ndefined class Performer\ndefined module Func\nimport Func._\ncache: com.datastax.spark.connector.rdd.CassandraTableScanRDD[Performer] \u003d CassandraTableScanRDD[46] at RDD at CassandraRDD.scala:15\n"
      },
      "dateCreated": "May 20, 2015 3:57:36 PM",
      "dateStarted": "Sep 23, 2015 10:51:52 AM",
      "dateFinished": "Sep 23, 2015 10:52:01 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1432130600982_-79430084",
      "id": "20150520-160320_901589394",
      "dateCreated": "May 20, 2015 4:03:20 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "05-Cassandra Spark For Data Cleaning",
  "id": "2AQ39SZ6F",
  "angularObjects": {
    "2ARKVF46K": [],
    "2ANYRH787": [],
    "2AQBHNCAB": [],
    "2ANRWDJG1": [],
    "2ANNAMCUB": [],
    "2ARR8D6R9": [],
    "2AQAS485Z": []
  },
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}